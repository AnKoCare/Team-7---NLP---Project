{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trinhquocan/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyvi in /Users/trinhquocan/Library/Python/3.9/lib/python/site-packages (0.1.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/trinhquocan/Library/Python/3.9/lib/python/site-packages (from pyvi) (1.6.0)\n",
      "Requirement already satisfied: sklearn-crfsuite in /Users/trinhquocan/Library/Python/3.9/lib/python/site-packages (from pyvi) (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/trinhquocan/Library/Python/3.9/lib/python/site-packages (from scikit-learn->pyvi) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/trinhquocan/Library/Python/3.9/lib/python/site-packages (from scikit-learn->pyvi) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/trinhquocan/Library/Python/3.9/lib/python/site-packages (from scikit-learn->pyvi) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/trinhquocan/Library/Python/3.9/lib/python/site-packages (from scikit-learn->pyvi) (3.5.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.7 in /Users/trinhquocan/Library/Python/3.9/lib/python/site-packages (from sklearn-crfsuite->pyvi) (0.9.11)\n",
      "Requirement already satisfied: tabulate>=0.4.2 in /Users/trinhquocan/Library/Python/3.9/lib/python/site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in /Users/trinhquocan/Library/Python/3.9/lib/python/site-packages (from sklearn-crfsuite->pyvi) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "!pip install pyvi\n",
    "from pyvi import ViTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang load word embedding tiếng Việt...\n"
     ]
    }
   ],
   "source": [
    "print(\"Đang load word embedding tiếng Việt...\")\n",
    "file_path = \"cc.vi.300.vec\" \n",
    "model_emb = KeyedVectors.load_word2vec_format(file_path, binary=False)\n",
    "\n",
    "embedding_dim = model_emb.vector_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in model_emb.key_to_index:\n",
    "        return model_emb[word]\n",
    "    else:\n",
    "        return np.zeros(embedding_dim, dtype=np.float32)\n",
    "\n",
    "def preprocess_vi(sentence):\n",
    "    s = sentence.lower().strip()\n",
    "    # tokenize\n",
    "    s = ViTokenizer.tokenize(s) \n",
    "    tokens = s.split()\n",
    "    return tokens\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    if norm_a < 1e-9 or norm_b < 1e-9:\n",
    "        return 0.0\n",
    "    return float((a @ b) / (norm_a * norm_b))\n",
    "\n",
    "def find_best_match(vec_s, list_vec_t):\n",
    "    # Tìm t_j có sim cao nhất\n",
    "    best_sim = -1.0\n",
    "    best_vec = np.zeros_like(vec_s)\n",
    "    for vec_t in list_vec_t:\n",
    "        sim = cosine_sim(vec_s, vec_t)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_vec = vec_t\n",
    "    return best_vec\n",
    "\n",
    "def linear_decompose(s_i, s_i_hat):\n",
    "    alpha = cosine_sim(s_i, s_i_hat)\n",
    "    s_plus = alpha * s_i\n",
    "    s_minus = (1 - alpha) * s_i\n",
    "    return s_plus, s_minus\n",
    "\n",
    "\n",
    "class SimilarityDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sent1 = str(row[\"sentence1\"])\n",
    "        sent2 = str(row[\"sentence2\"])\n",
    "        label = float(row[\"similarity\"])  # range [0..1]\n",
    "        return sent1, sent2, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # match_len: fix 20 (toy)\n",
    "    max_len = 20\n",
    "\n",
    "    batch_similar = []\n",
    "    batch_dissimilar = []\n",
    "    batch_labels = []\n",
    "\n",
    "    for sent1, sent2, label in batch:\n",
    "        tokens_s = preprocess_vi(sent1)\n",
    "        tokens_t = preprocess_vi(sent2)\n",
    "\n",
    "        vecs_s = [get_vector(tok) for tok in tokens_s]\n",
    "        vecs_t = [get_vector(tok) for tok in tokens_t]\n",
    "\n",
    "        s_plus_list = []\n",
    "        s_minus_list = []\n",
    "        for s_i in vecs_s:\n",
    "            s_i_hat = find_best_match(s_i, vecs_t)\n",
    "            s_plus, s_minus = linear_decompose(s_i, s_i_hat)\n",
    "            s_plus_list.append(s_plus)\n",
    "            s_minus_list.append(s_minus)\n",
    "\n",
    "        # Truncate/pad\n",
    "        s_plus_list = s_plus_list[:max_len]\n",
    "        s_minus_list = s_minus_list[:max_len]\n",
    "        pad_len = max_len - len(s_plus_list)\n",
    "\n",
    "        s_plus_list += [np.zeros(embedding_dim, dtype=np.float32)] * pad_len\n",
    "        s_minus_list += [np.zeros(embedding_dim, dtype=np.float32)] * pad_len\n",
    "\n",
    "        plus_array = np.stack(s_plus_list, axis=0)\n",
    "        minus_array = np.stack(s_minus_list, axis=0)\n",
    "\n",
    "        batch_similar.append(plus_array)\n",
    "        batch_dissimilar.append(minus_array)\n",
    "        batch_labels.append(label)\n",
    "\n",
    "    sim_tensor = torch.tensor(batch_similar, dtype=torch.float)\n",
    "    dis_tensor = torch.tensor(batch_dissimilar, dtype=torch.float)\n",
    "\n",
    "    # Ghép 2 channel: (B, 2, max_len, emb_dim)\n",
    "    input_tensor = torch.stack([sim_tensor, dis_tensor], dim=1)\n",
    "    labels_tensor = torch.tensor(batch_labels, dtype=torch.float).view(-1,1)\n",
    "\n",
    "    return input_tensor, labels_tensor\n",
    "\n",
    "\n",
    "class TwoChannelCNN(nn.Module):\n",
    "    def __init__(self, emb_dim, num_filters=64, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=2,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=(kernel_size, kernel_size),\n",
    "            padding=(1,1)\n",
    "        )\n",
    "        self.pool = nn.AdaptiveMaxPool2d((1,1))\n",
    "        self.fc = nn.Linear(num_filters, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, 2, max_len, emb_dim)\n",
    "        feat = self.conv(x)               # => (B, num_filters, ?, ?)\n",
    "        feat = nn.functional.relu(feat)\n",
    "        feat = self.pool(feat)            # => (B, num_filters, 1,1)\n",
    "        feat = feat.squeeze(-1).squeeze(-1) # => (B, num_filters)\n",
    "        out = self.fc(feat)               # => (B,1)\n",
    "        out = self.sigmoid(out)           # => (B,1), range [0..1]\n",
    "        return out\n",
    "\n",
    "# 6) ĐỌC DỮ LIỆU & TRAIN\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # sentence1, sentence2, similarity\n",
    "    # filtered_output.csv\n",
    "    # normalized_converted_data.csv\n",
    "    # data_main.csv\n",
    "    df_all = pd.read_csv(\"data_train_main.csv\", sep=\";\", decimal=\",\")\n",
    "\n",
    "\n",
    "    # Shuffle + tách train/val\n",
    "    df_shuf = df_all.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    train_size = int(0.8 * len(df_shuf))\n",
    "    df_train = df_shuf[:train_size]\n",
    "    df_val   = df_shuf[train_size:]\n",
    "\n",
    "    train_ds = SimilarityDataset(df_train)\n",
    "    val_ds   = SimilarityDataset(df_val)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Training device:\", device)\n",
    "\n",
    "    model = TwoChannelCNN(emb_dim=embedding_dim, num_filters=64, kernel_size=3).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    def eval_loss(dloader):\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dloader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                pred = model(xb)\n",
    "                loss = criterion(pred, yb)\n",
    "                losses.append(loss.item())\n",
    "        return np.mean(losses)\n",
    "\n",
    "    num_epochs = 10  # tùy chọn\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_l = eval_loss(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"TrainLoss={np.mean(train_losses):.4f}, \"\n",
    "              f\"ValLoss={val_l:.4f}\")\n",
    "\n",
    "    # Lưu trọng số\n",
    "    torch.save(model.state_dict(), \"model_weights_vi_6.pt\")\n",
    "    print(\"Đã train xong. Mô hình được lưu vào model_weights_vi.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "\n",
    "def predict_similarity(model, sentence1, sentence2, max_len=20):\n",
    "    # 1) Preprocess\n",
    "    tokens_s = preprocess_vi(sentence1)\n",
    "    tokens_t = preprocess_vi(sentence2)\n",
    "    \n",
    "    # 2) Embed\n",
    "    vecs_s = [get_vector(w) for w in tokens_s]\n",
    "    vecs_t = [get_vector(w) for w in tokens_t]\n",
    "    \n",
    "    # 3) Matching + Decomposition\n",
    "    s_plus_list = []\n",
    "    s_minus_list = []\n",
    "    for s_i in vecs_s:\n",
    "        s_i_hat = find_best_match(s_i, vecs_t)\n",
    "        s_plus, s_minus = linear_decompose(s_i, s_i_hat)\n",
    "        s_plus_list.append(s_plus)\n",
    "        s_minus_list.append(s_minus)\n",
    "    \n",
    "    # 4) Truncate / pad cho đủ max_len\n",
    "    s_plus_list  = s_plus_list[:max_len]\n",
    "    s_minus_list = s_minus_list[:max_len]\n",
    "    \n",
    "    pad_len = max_len - len(s_plus_list)\n",
    "    s_plus_list  += [np.zeros(embedding_dim, dtype=np.float32)] * pad_len\n",
    "    s_minus_list += [np.zeros(embedding_dim, dtype=np.float32)] * pad_len\n",
    "    \n",
    "    # (seq_len, emb_dim) => stack => (max_len, emb_dim)\n",
    "    plus_array  = np.stack(s_plus_list, axis=0)\n",
    "    minus_array = np.stack(s_minus_list, axis=0)\n",
    "    \n",
    "    # 5) Ghép 2 kênh => shape (1,2,max_len,emb_dim)\n",
    "    sim_tensor = torch.tensor([plus_array], dtype=torch.float)\n",
    "    dis_tensor = torch.tensor([minus_array], dtype=torch.float)\n",
    "    \n",
    "    input_tensor = torch.stack([sim_tensor, dis_tensor], dim=1)\n",
    "    \n",
    "    # 6) Đưa vào model -> ra similarity\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    # output shape (1,1)\n",
    "    sim_score = float(output.item())\n",
    "    return sim_score\n",
    "\n",
    "def predict_labels(similarity_scores, threshold=0.5):\n",
    "    return [1 if sim >= threshold else 0 for sim in similarity_scores]\n",
    "\n",
    "def evaluate_model(model, ground_truth_file, threshold=0.5):\n",
    "    data = pd.read_csv(ground_truth_file, delimiter=';', decimal=',')\n",
    "    \n",
    "    sentence1 = data['sentence1'].tolist()\n",
    "    sentence2 = data['sentence2'].tolist()\n",
    "    true_similarity = data['similarity'].tolist()\n",
    "\n",
    "    predicted_similarity = []\n",
    "    for sentA, sentB in zip(sentence1, sentence2):\n",
    "        similarity_value = predict_similarity(model, sentA, sentB)\n",
    "        predicted_similarity.append(similarity_value)\n",
    "\n",
    "    true_labels = predict_labels(true_similarity, threshold)\n",
    "    predicted_labels = predict_labels(predicted_similarity, threshold)\n",
    "\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    model = TwoChannelCNN(emb_dim=embedding_dim, num_filters=64, kernel_size=3).to(device)\n",
    "    model.load_state_dict(torch.load(\"model_weights_vi_5.pt\", map_location=device))\n",
    "\n",
    "    ground_truth_file = \"normalized_converted_data.csv\"  \n",
    "\n",
    "    precision, recall, f1 = evaluate_model(model, ground_truth_file, threshold)\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Measure: {f1:.4f}\")\n",
    "\n",
    "    # threshold = 0.7\n",
    "    # model_weights_vi Precision: 0.8135 Recall: 0.2836 F1-Measure: 0.4206\n",
    "    # model_weights_vi_1 Precision: 0.7036 Recall: 0.7582 F1-Measure: 0.7299\n",
    "    # model_weights_vi_2 Precision: 0.7317 Recall: 0.6526 F1-Measure: 0.6899\n",
    "    # model_weights_vi_3 Precision: 0.6122 Recall: 0.9713 F1-Measure: 0.7510\n",
    "    # model_weights_vi_4 Precision: 0.6845 Recall: 0.8729 F1-Measure: 0.7673\n",
    "    # model_weights_vi_5 Precision: 0.6882 Recall: 0.8800 F1-Measure: 0.7724\n",
    "    # model_weights_vi_6 Precision: 0.7903 Recall: 0.4824 F1-Measure: 0.5991\n",
    "    # model_weights_vi_7 Precision: 0.7218 Recall: 0.7611 F1-Measure: 0.7409\n",
    "    # model_weights_vi_5k Precision: 0.6996 Recall: 0.5189 F1-Measure: 0.5959\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
